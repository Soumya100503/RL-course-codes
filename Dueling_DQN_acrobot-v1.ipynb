{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class duelling_dqn_model(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64,  vl1_units = 256, al1_units = 256, type_num = 1):\n",
    "        super(duelling_dqn_model, self).__init__()\n",
    "        self.type_num = type_num\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.value_layer_1 = nn.Linear(fc1_units, vl1_units)\n",
    "        self.advantage_layer_1 = nn.Linear(fc1_units, al1_units)\n",
    "\n",
    "        self.value_layer_2 = nn.Linear(vl1_units, 1)\n",
    "        self.advantage_layer_2 = nn.Linear(al1_units, action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        a=F.relu(self.fc1(state))\n",
    "        adv=F.relu(self.advantage_layer_1(a))\n",
    "        adv=self.advantage_layer_2(adv)\n",
    "        value=F.relu(self.value_layer_1(a))\n",
    "        value=self.value_layer_2(value) \n",
    "        advAverage = torch.mean(adv, dim=1, keepdim=True)\n",
    "        Q = value + adv - advAverage\n",
    "        return Q\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            Q = self.forward(state)\n",
    "            action_index = torch.argmax(Q, dim=1)\n",
    "        return action_index.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, memory_size: int) -> None:\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=self.memory_size)\n",
    "\n",
    "    def add(self, experience) -> None:\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size: int, continuous: bool = True):\n",
    "        if batch_size > len(self.buffer):\n",
    "            batch_size = len(self.buffer)\n",
    "        if continuous:\n",
    "            rand = random.randint(0, len(self.buffer) - batch_size)\n",
    "            return [self.buffer[i] for i in range(rand, rand + batch_size)]\n",
    "        else:\n",
    "            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "            return [self.buffer[i] for i in indexes]\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial,env,type_num):\n",
    "    # Define the search space\n",
    "    params= {\n",
    "      'batch_size' : trial.suggest_categorical('batch_size', [32,64,128]),\n",
    "      'lr'  : trial.suggest_loguniform('lr',1e-5,1e-4),\n",
    "      'eps_start' : trial.suggest_loguniform('eps_start',0.1,0.2),\n",
    "      'replay_size':trial.suggest_categorical('replay_size', [50000,75000,100000]),\n",
    "    }\n",
    "\n",
    "    # Train the model with the given hyperparameters\n",
    "    seed=1\n",
    "    rewards_episode= duel_dqn(env,seed = seed,params = params,type_num = type_num)\n",
    "    return np.mean(rewards_episode)\n",
    "\n",
    "def duel_dqn(env,seed,params,type_num):\n",
    "    print('\\n')\n",
    "    print(\"For seed =\",seed)\n",
    "    env = env\n",
    "    env.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    qnetwork_local = duelling_dqn_model(state_shape, action_shape,seed,type_num=type_num).to(device)\n",
    "    qnetwork_target = duelling_dqn_model(state_shape, action_shape,seed,type_num=type_num).to(device)\n",
    "    \n",
    "    qnetwork_target.load_state_dict(qnetwork_local.state_dict())\n",
    "    optimizer = torch.optim.Adam(qnetwork_local.parameters(), lr=params['lr'])\n",
    "\n",
    "\n",
    "    GAMMA = 0.99\n",
    "    EXPLORE = 20000\n",
    "    eps_start = params['eps_start']\n",
    "    eps_end = 0.0001\n",
    "    REPLAY_MEMORY = params['replay_size']\n",
    "    BATCH = params['batch_size']\n",
    "    max_episodes = 500\n",
    "    UPDATE_STEPS = 4\n",
    "\n",
    "    memory_replay = Memory(REPLAY_MEMORY)\n",
    "\n",
    "    epsilon = eps_start\n",
    "    learn_steps = 0\n",
    "    begin_learn = False\n",
    "    scores_window = deque(maxlen=100)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for epoch in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            prob = random.random()\n",
    "            if prob < epsilon:\n",
    "                action = random.choice(np.arange(action_shape))\n",
    "            else:\n",
    "                tensor_state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action = qnetwork_local.select_action(tensor_state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            memory_replay.add((state, next_state, action, reward, done))\n",
    "            # Cheks if the replay buffer has enough samples to sample from\n",
    "            if memory_replay.size() > 128:\n",
    "                if not begin_learn:\n",
    "                    begin_learn = True\n",
    "                learn_steps += 1\n",
    "                if learn_steps % UPDATE_STEPS == 0:\n",
    "                    qnetwork_target.load_state_dict(qnetwork_local.state_dict())\n",
    "                # Sampling batch size number of samples for target network\n",
    "                batch = memory_replay.sample(BATCH, False)\n",
    "                batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*batch)\n",
    "\n",
    "                batch_state = torch.FloatTensor(batch_state).to(device)\n",
    "                batch_next_state = torch.FloatTensor(batch_next_state).to(device)\n",
    "                batch_action = torch.FloatTensor(batch_action).unsqueeze(1).to(device)\n",
    "                batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
    "                batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    localQ_next = qnetwork_local(batch_next_state)\n",
    "                    targetQ_next = qnetwork_target(batch_next_state)\n",
    "                    local_max_action = torch.argmax(localQ_next, dim=1, keepdim=True)\n",
    "                    y = batch_reward + (1 - batch_done) * GAMMA * targetQ_next.gather(1, local_max_action.long())\n",
    "\n",
    "                loss = F.mse_loss(qnetwork_local(batch_state).gather(1, batch_action.long()), y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epsilon > eps_end:\n",
    "                    epsilon -= (eps_start - eps_end) / EXPLORE\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        scores_window.append(episode_reward)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        print('\\rEpisode {}\\tMoving Average Score: {:.2f}'.format(epoch, np.mean(scores_window)), end=\"\")\n",
    "        if epoch % 100 == 0:\n",
    "            print('\\rEpisode {}\\tMoving Average Score: {:.2f}'.format(epoch, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=env.spec.reward_threshold and epoch >= 100:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverageScore: {:.2f}'.format(epoch, np.mean(scores_window)))\n",
    "            break\n",
    "\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\beher\\anaconda3\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\beher\\anaconda3\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 04:52:12,015] A new study created in memory with name: no-name-3fe16fa7-2c22-4869-b8cd-48ac6ccfcc7c\n",
      "C:\\Users\\beher\\AppData\\Local\\Temp\\ipykernel_29672\\3606942755.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lr'  : trial.suggest_loguniform('lr',1e-5,1e-4),\n",
      "C:\\Users\\beher\\AppData\\Local\\Temp\\ipykernel_29672\\3606942755.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'eps_start' : trial.suggest_loguniform('eps_start',0.1,0.2),\n",
      "c:\\Users\\beher\\anaconda3\\lib\\site-packages\\gym\\core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For seed = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\beher\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\beher\\AppData\\Local\\Temp\\ipykernel_29672\\3606942755.py:76: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  batch_state = torch.FloatTensor(batch_state).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -144.82\n",
      "Episode 157\tMoving Average Score: -100.03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 04:55:34,570] Trial 0 finished with value: -130.03773584905662 and parameters: {'batch_size': 128, 'lr': 6.415318655621856e-05, 'eps_start': 0.11076012404423284, 'replay_size': 100000}. Best is trial 0 with value: -130.03773584905662.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 158\tMoving Average Score: -99.94\n",
      "Environment solved in 158 episodes!\tAverageScore: -99.94\n",
      "\n",
      "\n",
      "For seed = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beher\\AppData\\Local\\Temp\\ipykernel_29672\\3606942755.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lr'  : trial.suggest_loguniform('lr',1e-5,1e-4),\n",
      "C:\\Users\\beher\\AppData\\Local\\Temp\\ipykernel_29672\\3606942755.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'eps_start' : trial.suggest_loguniform('eps_start',0.1,0.2),\n",
      "c:\\Users\\beher\\anaconda3\\lib\\site-packages\\gym\\core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -391.48\n",
      "Episode 200\tMoving Average Score: -335.92\n",
      "Episode 300\tMoving Average Score: -405.61\n",
      "Episode 400\tMoving Average Score: -224.31\n",
      "Episode 498\tMoving Average Score: -143.40"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 05:23:28,927] Trial 1 finished with value: -300.72 and parameters: {'batch_size': 128, 'lr': 1.1362130880884927e-05, 'eps_start': 0.12007214704516453, 'replay_size': 50000}. Best is trial 0 with value: -130.03773584905662.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 499\tMoving Average Score: -143.27\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -180.01\n",
      "Episode 200\tMoving Average Score: -106.27\n",
      "Episode 300\tMoving Average Score: -103.50\n",
      "Episode 325\tMoving Average Score: -102.97"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 05:29:36,421] Trial 2 finished with value: -127.96636085626912 and parameters: {'batch_size': 64, 'lr': 4.578448201987797e-05, 'eps_start': 0.17365180632399785, 'replay_size': 50000}. Best is trial 2 with value: -127.96636085626912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 326\tMoving Average Score: -99.13\n",
      "Environment solved in 326 episodes!\tAverageScore: -99.13\n",
      "\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -154.27\n",
      "Episode 200\tMoving Average Score: -108.84\n",
      "Episode 262\tMoving Average Score: -100.04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 05:34:02,378] Trial 3 finished with value: -124.48863636363636 and parameters: {'batch_size': 64, 'lr': 4.294928486327954e-05, 'eps_start': 0.12233771624701863, 'replay_size': 100000}. Best is trial 3 with value: -124.48863636363636.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 263\tMoving Average Score: -99.90\n",
      "Environment solved in 263 episodes!\tAverageScore: -99.90\n",
      "\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -199.19\n",
      "Episode 200\tMoving Average Score: -161.57\n",
      "Episode 300\tMoving Average Score: -121.02\n",
      "Episode 400\tMoving Average Score: -118.66\n",
      "Episode 491\tMoving Average Score: -100.60"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 05:44:36,958] Trial 4 finished with value: -141.46653144016227 and parameters: {'batch_size': 64, 'lr': 2.0009680959452588e-05, 'eps_start': 0.10895730379224866, 'replay_size': 100000}. Best is trial 3 with value: -124.48863636363636.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 492\tMoving Average Score: -99.97\n",
      "Environment solved in 492 episodes!\tAverageScore: -99.97\n",
      "Best trial:\n",
      "  Value:  -124.48863636363636\n",
      "  Params: \n",
      "    batch_size: 64\n",
      "    lr: 4.294928486327954e-05\n",
      "    eps_start: 0.12233771624701863\n",
      "    replay_size: 100000\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "#Run the optimization\n",
    "study.optimize(lambda trial: objective(trial, env, 1), n_trials=5)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For seed = 0\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -470.49\n",
      "Episode 200\tMoving Average Score: -132.06\n",
      "Episode 300\tMoving Average Score: -102.13\n",
      "Episode 329\tMoving Average Score: -99.529\n",
      "Environment solved in 329 episodes!\tAverageScore: -99.52\n",
      "\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -154.27\n",
      "Episode 200\tMoving Average Score: -108.84\n",
      "Episode 263\tMoving Average Score: -99.904\n",
      "Environment solved in 263 episodes!\tAverageScore: -99.90\n",
      "\n",
      "\n",
      "For seed = 2\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -415.58\n",
      "Episode 200\tMoving Average Score: -153.75\n",
      "Episode 300\tMoving Average Score: -128.76\n",
      "Episode 400\tMoving Average Score: -130.34\n",
      "Episode 499\tMoving Average Score: -104.10\n",
      "\n",
      "For seed = 3\n",
      "Episode 0\tMoving Average Score: -142.00\n",
      "Episode 100\tMoving Average Score: -148.11\n",
      "Episode 200\tMoving Average Score: -109.14\n",
      "Episode 230\tMoving Average Score: -99.732\n",
      "Environment solved in 230 episodes!\tAverageScore: -99.73\n",
      "\n",
      "\n",
      "For seed = 4\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -498.02\n",
      "Episode 200\tMoving Average Score: -321.01\n",
      "Episode 300\tMoving Average Score: -204.69\n",
      "Episode 400\tMoving Average Score: -174.99\n",
      "Episode 499\tMoving Average Score: -153.97"
     ]
    }
   ],
   "source": [
    "num_seeds = 5\n",
    "all_episode_rewards = []\n",
    "\n",
    "for seed in range(num_seeds):\n",
    "    episode_rewards = duel_dqn(env,seed = seed,params = trial.params,type_num = 1)\n",
    "    all_episode_rewards.append(episode_rewards)\n",
    "\n",
    "# Calculate mean and variance across runs for each episode\n",
    "max_length = max(len(v) for v in all_episode_rewards)\n",
    "padded_rewards = [np.pad(v, (0, max_length - len(v)), mode='constant',constant_values = env.spec.reward_threshold) for v in all_episode_rewards]\n",
    "mean_rewards_acro_1 = np.mean(padded_rewards, axis=0)\n",
    "variance_rewards_acro_1 = np.var(padded_rewards, axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for type 2\n",
    "class duelling_dqn_model(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64,  vl1_units = 256, al1_units = 256, type_num = 1):\n",
    "        super(duelling_dqn_model, self).__init__()\n",
    "        self.type_num = type_num\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.value_layer_1 = nn.Linear(fc1_units, vl1_units)\n",
    "        self.advantage_layer_1 = nn.Linear(fc1_units, al1_units)\n",
    "\n",
    "        self.value_layer_2 = nn.Linear(vl1_units, 1)\n",
    "        self.advantage_layer_2 = nn.Linear(al1_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        a=F.relu(self.fc1(state))\n",
    "        adv=F.relu(self.advantage_layer_1(a))\n",
    "        adv=self.advantage_layer_2(adv)\n",
    "        value=F.relu(self.value_layer_1(a))\n",
    "        value=self.value_layer_2(value) \n",
    "        advMax = torch.max(adv, dim=1, keepdim=True).values\n",
    "        Q = value + adv - advMax\n",
    "        return Q\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            Q = self.forward(state)\n",
    "            action_index = torch.argmax(Q, dim=1)\n",
    "        return action_index.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 06:36:04,078] A new study created in memory with name: no-name-3c8c12ec-b22a-4373-acde-af67384da5e3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For seed = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beher\\AppData\\Local\\Temp\\ipykernel_29672\\3606942755.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lr'  : trial.suggest_loguniform('lr',1e-5,1e-4),\n",
      "C:\\Users\\beher\\AppData\\Local\\Temp\\ipykernel_29672\\3606942755.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'eps_start' : trial.suggest_loguniform('eps_start',0.1,0.2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tMoving Average Score: -220.00\n",
      "Episode 100\tMoving Average Score: -183.68\n",
      "Episode 200\tMoving Average Score: -104.52\n",
      "Episode 254\tMoving Average Score: -100.47"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 06:39:22,329] Trial 0 finished with value: -134.8046875 and parameters: {'batch_size': 64, 'lr': 4.5330796683795955e-05, 'eps_start': 0.14905763894045493, 'replay_size': 50000}. Best is trial 0 with value: -134.8046875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 255\tMoving Average Score: -98.99\n",
      "Environment solved in 255 episodes!\tAverageScore: -98.99\n",
      "\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -180.00\n",
      "Episode 100\tMoving Average Score: -349.82\n",
      "Episode 200\tMoving Average Score: -147.25\n",
      "Episode 300\tMoving Average Score: -181.57\n",
      "Episode 400\tMoving Average Score: -125.82\n",
      "Episode 498\tMoving Average Score: -114.97"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 06:49:19,038] Trial 1 finished with value: -184.102 and parameters: {'batch_size': 64, 'lr': 1.3325577486801331e-05, 'eps_start': 0.1040215299417645, 'replay_size': 50000}. Best is trial 0 with value: -134.8046875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 499\tMoving Average Score: -115.18\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -182.00\n",
      "Episode 100\tMoving Average Score: -150.76\n",
      "Episode 200\tMoving Average Score: -111.48\n",
      "Episode 275\tMoving Average Score: -100.77"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 06:52:09,638] Trial 2 finished with value: -120.8158844765343 and parameters: {'batch_size': 32, 'lr': 9.748005267572618e-05, 'eps_start': 0.11779140743708955, 'replay_size': 75000}. Best is trial 2 with value: -120.8158844765343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 276\tMoving Average Score: -99.84\n",
      "Environment solved in 276 episodes!\tAverageScore: -99.84\n",
      "\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -201.54\n",
      "Episode 200\tMoving Average Score: -145.49\n",
      "Episode 300\tMoving Average Score: -103.43\n",
      "Episode 400\tMoving Average Score: -106.74\n",
      "Episode 404\tMoving Average Score: -101.41"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 06:57:19,174] Trial 3 finished with value: -139.45320197044336 and parameters: {'batch_size': 32, 'lr': 5.528522998056511e-05, 'eps_start': 0.10043594494743395, 'replay_size': 100000}. Best is trial 2 with value: -120.8158844765343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 405\tMoving Average Score: -97.62\n",
      "Environment solved in 405 episodes!\tAverageScore: -97.62\n",
      "\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -192.00\n",
      "Episode 100\tMoving Average Score: -216.16\n",
      "Episode 197\tMoving Average Score: -100.02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 07:00:16,759] Trial 4 finished with value: -158.8391959798995 and parameters: {'batch_size': 64, 'lr': 1.5369142177490045e-05, 'eps_start': 0.19310917391448384, 'replay_size': 50000}. Best is trial 2 with value: -120.8158844765343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 198\tMoving Average Score: -99.71\n",
      "Environment solved in 198 episodes!\tAverageScore: -99.71\n",
      "Best trial:\n",
      "  Value:  -120.8158844765343\n",
      "  Params: \n",
      "    batch_size: 32\n",
      "    lr: 9.748005267572618e-05\n",
      "    eps_start: 0.11779140743708955\n",
      "    replay_size: 75000\n"
     ]
    }
   ],
   "source": [
    "study_2 = optuna.create_study(direction='maximize')\n",
    "\n",
    "study_2.optimize(lambda trial: objective(trial,env,2), n_trials=5)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best trial:\")\n",
    "trial_2 = study_2.best_trial\n",
    "print(\"  Value: \", trial_2.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial_2.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For seed = 0\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -197.68\n",
      "Episode 200\tMoving Average Score: -112.98\n",
      "Episode 300\tMoving Average Score: -125.09\n",
      "Episode 400\tMoving Average Score: -103.78\n",
      "Episode 428\tMoving Average Score: -99.786\n",
      "Environment solved in 428 episodes!\tAverageScore: -99.78\n",
      "\n",
      "\n",
      "For seed = 1\n",
      "Episode 0\tMoving Average Score: -182.00\n",
      "Episode 100\tMoving Average Score: -150.76\n",
      "Episode 200\tMoving Average Score: -111.48\n",
      "Episode 276\tMoving Average Score: -99.847\n",
      "Environment solved in 276 episodes!\tAverageScore: -99.84\n",
      "\n",
      "\n",
      "For seed = 2\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -204.05\n",
      "Episode 170\tMoving Average Score: -99.798\n",
      "Environment solved in 170 episodes!\tAverageScore: -99.79\n",
      "\n",
      "\n",
      "For seed = 3\n",
      "Episode 0\tMoving Average Score: -142.00\n",
      "Episode 100\tMoving Average Score: -159.19\n",
      "Episode 200\tMoving Average Score: -114.28\n",
      "Episode 251\tMoving Average Score: -99.917\n",
      "Environment solved in 251 episodes!\tAverageScore: -99.91\n",
      "\n",
      "\n",
      "For seed = 4\n",
      "Episode 0\tMoving Average Score: -500.00\n",
      "Episode 100\tMoving Average Score: -217.78\n",
      "Episode 200\tMoving Average Score: -103.04\n",
      "Episode 216\tMoving Average Score: -99.860\n",
      "Environment solved in 216 episodes!\tAverageScore: -99.86\n"
     ]
    }
   ],
   "source": [
    "num_seeds = 5\n",
    "all_episode_rewards = []\n",
    "\n",
    "for seed in range(num_seeds):\n",
    "    episode_rewards = duel_dqn(env,seed = seed,params = trial_2.params,type_num = 2)\n",
    "    all_episode_rewards.append(episode_rewards)\n",
    "\n",
    "# Calculate mean and variance across runs for each episode\n",
    "max_length = max(len(v) for v in all_episode_rewards)\n",
    "padded_rewards = [np.pad(v, (0, max_length - len(v)), mode='constant',constant_values = env.spec.reward_threshold) for v in all_episode_rewards]\n",
    "mean_rewards_acro_2 = np.mean(padded_rewards, axis=0)\n",
    "variance_rewards_acro_2 = np.var(padded_rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(mean_rewards_acro_1) + 1), mean_rewards_acro_1, label='Mean Return for type 1(using mean)', color='blue')\n",
    "plt.fill_between(range(1, len(mean_rewards_acro_1) + 1), mean_rewards_acro_1 - np.sqrt(variance_rewards_acro_1),\n",
    "                 mean_rewards_acro_1 + np.sqrt(variance_rewards_acro_1), color='blue', alpha=0.2, label='Variance for type 1(using mean)')\n",
    "plt.plot(range(1, len(mean_rewards_acro_2) + 1), mean_rewards_acro_2, label='Mean Return for type 2(using max)', color='orange')\n",
    "plt.fill_between(range(1, len(mean_rewards_acro_2) + 1), mean_rewards_acro_2 - np.sqrt(variance_rewards_acro_2),\n",
    "                 mean_rewards_acro_2 + np.sqrt(variance_rewards_acro_2), color='orange', alpha=0.2, label='Variance for type 2(using max)')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episodic Return')\n",
    "plt.title('Episodic Return vs. Episode Number (Mean and Variance across 5 seeds)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
